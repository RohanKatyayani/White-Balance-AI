{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858be97-ca19-4f2b-a740-05daeff484aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio\n",
    "!{sys.executable} -m pip install opencv-python pillow matplotlib pandas numpy scikit-learn\n",
    "!{sys.executable} -m pip install seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f13ea3-9212-4919-b71b-0dba8fed1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")  \n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\") \n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"Tensor test: {x + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8aec3-edc6-4a9e-8079-5f5bbefcc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_csv_path = os.path.join(dataset_path, 'Train', 'sliders.csv')\n",
    "validation_csv_path = os.path.join(dataset_path, 'Validation', 'sliders_input.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"Shape: {train_df.shape}\")  \n",
    "print(train_df.head())\n",
    "print(train_df.columns.tolist())\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "validation_df = pd.read_csv(validation_csv_path)\n",
    "print(f\"Shape: {validation_df.shape}\")\n",
    "print(validation_df.head())\n",
    "print(validation_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc993d2-4104-4414-aa99-9f800b0967bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(validation_df)}\")\n",
    "\n",
    "print(f\"Temperature range: {train_df['Temperature'].min()} to {train_df['Temperature'].max()}\")\n",
    "print(f\"Tint range: {train_df['Tint'].min()} to {train_df['Tint'].max()}\")\n",
    "\n",
    "print(f\"- Aperture: {train_df['aperture'].min()} to {train_df['aperture'].max()}\")\n",
    "print(f\"- Focal Length: {train_df['focalLength'].min()} to {train_df['focalLength'].max()}\")\n",
    "print(f\"- ISO: {train_df['isoSpeedRating'].min()} to {train_df['isoSpeedRating'].max()}\")\n",
    "\n",
    "print(f\"Current Temp (As Shot WB): {train_df['currTemp'].min()} to {train_df['currTemp'].max()}\")\n",
    "print(f\"Current Tint (As Shot WB): {train_df['currTint'].min()} to {train_df['currTint'].max()}\")\n",
    "\n",
    "train_images_path = os.path.join(dataset_path, 'Train', 'images')\n",
    "validation_images_path = os.path.join(dataset_path, 'Validation', 'images')\n",
    "\n",
    "print(f\"Train images folder exists: {os.path.exists(train_images_path)}\")\n",
    "print(f\"Validation images folder exists: {os.path.exists(validation_images_path)}\")\n",
    "\n",
    "if os.path.exists(train_images_path):\n",
    "    train_images = os.listdir(train_images_path)\n",
    "    print(f\"Number of train images: {len(train_images)}\")\n",
    "    print(f\"First 3 train images: {train_images[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778641b-3d78-4f3b-b32d-131ceb504b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install \"numpy<2\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed0a22-8e7c-42b9-a998-d08a2ed4932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Quick test\n",
    "print(\"Basic test:\", np.array([1, 2, 3]) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7e3a6-da88-4686-9f89-45d7f4bfb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteBalanceLoss(nn.Module):\n",
    "    def __init__(self, temp_weight=1.0, tint_weight=1.0):\n",
    "        super(WhiteBalanceLoss, self).__init__()\n",
    "        self.temp_weight = temp_weight\n",
    "        self.tint_weight = tint_weight\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        pred_temp, pred_tint = predictions[:, 0], predictions[:, 1]\n",
    "        true_temp, true_tint = targets[:, 0], targets[:, 1]\n",
    "        \n",
    "        temp_weights = 1.0 / torch.sqrt(true_temp / 1000.0 + 1e-6)\n",
    "        temp_loss = torch.mean(temp_weights * (pred_temp - true_temp) ** 2)\n",
    "        \n",
    "        tint_loss = torch.mean((pred_tint - true_tint) ** 2)\n",
    "        \n",
    "        total_loss = self.temp_weight * temp_loss + self.tint_weight * tint_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "criterion = WhiteBalanceLoss()\n",
    "\n",
    "test_predictions = torch.tensor([[4150.0, 2.0], [3000.0, -5.0]])\n",
    "test_targets = torch.tensor([[4200.0, 1.0], [3100.0, -3.0]])\n",
    "\n",
    "loss = criterion(test_predictions, test_targets)\n",
    "print(f\"Loss function test: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b85112-c09b-4441-82a0-da8ad82e2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0  \n",
    ")\n",
    "\n",
    "print(f\"Training DataLoader created:\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Total batches: {len(train_loader)}\")\n",
    "print(f\"- Samples per epoch: {len(train_dataset)}\")\n",
    "\n",
    "for images, metadata, targets in train_loader:\n",
    "    print(f\"Batch test successful:\")\n",
    "    print(f\"- Images shape: {images.shape}\")\n",
    "    print(f\"- Metadata shape: {metadata.shape}\") \n",
    "    print(f\"- Targets shape: {targets.shape}\")\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56c233-dcac-4f98-b1be-5d230a271975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dataset_path = 'enter your data path'\n",
    "train_df = pd.read_csv(os.path.join(dataset_path, 'Train', 'sliders.csv'))\n",
    "\n",
    "def scale_targets(targets):\n",
    "    \"\"\"Scale Temperature and Tint to reasonable ranges\"\"\"\n",
    "    temp_scaled = targets[:, 0] / 1000.0  # Scale Temperature from 2000-50000 to 2-50\n",
    "    tint_scaled = targets[:, 1] / 10.0    # Scale Tint from -150-150 to -15-15\n",
    "    return torch.stack([temp_scaled, tint_scaled], dim=1)\n",
    "\n",
    "def unscale_predictions(predictions):\n",
    "    \"\"\"Convert scaled predictions back to original range\"\"\"\n",
    "    temp_original = predictions[:, 0] * 1000.0\n",
    "    tint_original = predictions[:, 1] * 10.0\n",
    "    return torch.stack([temp_original, tint_original], dim=1)\n",
    "\n",
    "test_targets = torch.tensor([[4150.0, 2.0], [2000.0, -50.0]])\n",
    "scaled = scale_targets(test_targets)\n",
    "unscaled = unscale_predictions(scaled)\n",
    "\n",
    "print(f\"Scaling test:\")\n",
    "print(f\"Original: {test_targets[0].numpy()}\")\n",
    "print(f\"Scaled: {scaled[0].numpy()}\")\n",
    "print(f\"Unscaled: {unscaled[0].numpy()}\")\n",
    "\n",
    "print(f\"\\nDATA DISTRIBUTION ANALYSIS:\")\n",
    "print(f\"Temperature - Min: {train_df['Temperature'].min()}, Max: {train_df['Temperature'].max()}\")\n",
    "print(f\"Tint - Min: {train_df['Tint'].min()}, Max: {train_df['Tint'].max()}\")\n",
    "print(f\"As Shot Temp - Min: {train_df['currTemp'].min()}, Max: {train_df['currTemp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b38f9-f7ac-426c-ad76-89bfa62e4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedWhiteBalanceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedWhiteBalanceModel, self).__init__()\n",
    "        \n",
    "        self.backbone = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        image_feature_dim = 1280\n",
    "        \n",
    "        self.metadata_fc = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim + 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "        \n",
    "    def forward(self, image, metadata):\n",
    "        image_features = self.backbone(image)\n",
    "        metadata_features = self.metadata_fc(metadata)\n",
    "        combined_features = torch.cat([image_features, metadata_features], dim=1)\n",
    "        output = self.combined_fc(combined_features)\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "improved_model = ImprovedWhiteBalanceModel().to(device)\n",
    "improved_optimizer = torch.optim.AdamW(improved_model.parameters(), lr=0.0005)  \n",
    "\n",
    "improved_model.eval()\n",
    "with torch.no_grad():\n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((384, 384)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    test_input = torch.randn(1, 3, 384, 384).to(device)\n",
    "    test_metadata = torch.randn(1, 8).to(device)\n",
    "    \n",
    "    test_pred = improved_model(test_input, test_metadata)\n",
    "    print(f\"Improved model output range: {test_pred.min().item():.3f} to {test_pred.max().item():.3f}\")\n",
    "    print(f\"After unscaling: Temp ~{test_pred[0,0].item()*1000:.0f}K, Tint ~{test_pred[0,1].item()*10:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b88401-c631-47cd-8318-12d1601a58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ScaledWhiteBalanceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledWhiteBalanceLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        pred_temp, pred_tint = predictions[:, 0], predictions[:, 1]\n",
    "        true_temp, true_tint = targets[:, 0], targets[:, 1]\n",
    "        \n",
    "        temp_weights = 1.0 / torch.sqrt(true_temp + 1.0)  \n",
    "        temp_loss = torch.mean(temp_weights * (pred_temp - true_temp) ** 2)\n",
    "        \n",
    "        tint_loss = torch.mean((pred_tint - true_tint) ** 2)\n",
    "        \n",
    "        return temp_loss + tint_loss\n",
    "\n",
    "improved_model.train()\n",
    "criterion = ScaledWhiteBalanceLoss()\n",
    "\n",
    "num_epochs = 3  \n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images, metadata, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        images = images.to(device)\n",
    "        metadata = metadata.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        scaled_targets = scale_targets(targets)\n",
    "        \n",
    "        improved_optimizer.zero_grad()\n",
    "        predictions = improved_model(images, metadata)\n",
    "        loss = criterion(predictions, scaled_targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        improved_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete:\")\n",
    "    print(f\"- Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"- Time: {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    improved_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_image, test_metadata, test_target = train_dataset[0]\n",
    "        test_image = test_image.unsqueeze(0).to(device)\n",
    "        test_metadata = test_metadata.unsqueeze(0).to(device)\n",
    "        \n",
    "        prediction = improved_model(test_image, test_metadata)\n",
    "        unscaled_pred = unscale_predictions(prediction)\n",
    "        \n",
    "        print(f\"- Sample Prediction:\")\n",
    "        print(f\"Predicted: Temp {unscaled_pred[0,0].item():.0f}K, Tint {unscaled_pred[0,1].item():.0f}\")\n",
    "        print(f\"Actual:    Temp {test_target[0].item():.0f}K, Tint {test_target[1].item():.0f}\")\n",
    "        print(f\"Error:     Temp {abs(unscaled_pred[0,0].item() - test_target[0].item()):.0f}K\")\n",
    "\n",
    "print(f\"\\nTRAINING COMPLETE!\")\n",
    "print(f\"Final losses: {train_losses}\")\n",
    "\n",
    "if len(train_losses) > 1 and train_losses[-1] < train_losses[0]:\n",
    "    print(\"SUCCESS: Model is learning! Loss decreased.\")\n",
    "else:\n",
    "    print(\"Loss not decreasing significantly - may need more epochs or tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afe2e5-84ce-487f-9f56-16523b83ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"\\nTesting on multiple samples:\")\n",
    "    for i in range(3):\n",
    "        test_image, test_metadata, test_target = train_dataset[i]\n",
    "        test_image = test_image.unsqueeze(0).to(device)\n",
    "        test_metadata = test_metadata.unsqueeze(0).to(device)\n",
    "        \n",
    "        prediction = improved_model(test_image, test_metadata)\n",
    "        unscaled_pred = unscale_predictions(prediction)\n",
    "        \n",
    "        print(f\"Sample {i}:\")\n",
    "        print(f\"  As Shot WB: Temp {test_metadata[0,0].item():.0f}K, Tint {test_metadata[0,1].item():.0f}\")\n",
    "        print(f\"  Predicted:  Temp {unscaled_pred[0,0].item():.0f}K, Tint {unscaled_pred[0,1].item():.0f}\")\n",
    "        print(f\"  Actual:     Temp {test_target[0].item():.0f}K, Tint {test_target[1].item():.0f}\")\n",
    "        print()\n",
    "\n",
    "def calculate_adjustments(targets, metadata):\n",
    "    temp_adjustment = (targets[:, 0] - metadata[:, 0]) / 1000.0  # Scale adjustment\n",
    "    tint_adjustment = (targets[:, 1] - metadata[:, 1]) / 10.0    # Scale adjustment\n",
    "    return torch.stack([temp_adjustment, tint_adjustment], dim=1)\n",
    "\n",
    "def apply_adjustments(predictions, metadata):\n",
    "    temp_absolute = predictions[:, 0] * 1000.0 + metadata[:, 0]\n",
    "    tint_absolute = predictions[:, 1] * 10.0 + metadata[:, 1]\n",
    "    return torch.stack([temp_absolute, tint_absolute], dim=1)\n",
    "\n",
    "test_targets = torch.tensor([[4150.0, 2.0], [3000.0, -5.0]])\n",
    "test_metadata = torch.tensor([[6317.0, 4.0], [4000.0, 10.0]])  # As Shot WB\n",
    "\n",
    "adjustments = calculate_adjustments(test_targets, test_metadata)\n",
    "reconstructed = apply_adjustments(adjustments, test_metadata)\n",
    "\n",
    "print(\"Adjustment approach test:\")\n",
    "print(f\"Original: {test_targets[0].numpy()}\")\n",
    "print(f\"As Shot:  {test_metadata[0].numpy()}\")\n",
    "print(f\"Adjustment: {adjustments[0].numpy()}\")\n",
    "print(f\"Reconstructed: {reconstructed[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c46f930-9558-4065-b386-ef0a481149d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustmentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdjustmentModel, self).__init__()\n",
    "        \n",
    "        self.backbone = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        image_feature_dim = 1280\n",
    "        \n",
    "        self.metadata_fc = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim + 64, 512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),  \n",
    "            nn.Tanh()  \n",
    "        )\n",
    "        \n",
    "    def forward(self, image, metadata):\n",
    "        image_features = self.backbone(image)\n",
    "        metadata_features = self.metadata_fc(metadata)\n",
    "        combined = torch.cat([image_features, metadata_features], dim=1)\n",
    "        adjustments = self.combined_fc(combined)\n",
    "        return adjustments\n",
    "\n",
    "print(\"Creating adjustment-based model...\")\n",
    "adjustment_model = AdjustmentModel().to(device)\n",
    "adjustment_optimizer = torch.optim.Adam(adjustment_model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in adjustment_model.parameters()):,} parameters\")\n",
    "\n",
    "adjustment_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_image, test_metadata, test_target = train_dataset[0]\n",
    "    test_image = test_image.unsqueeze(0).to(device)\n",
    "    test_metadata = test_metadata.unsqueeze(0).to(device)\n",
    "    \n",
    "    test_pred = adjustment_model(test_image, test_metadata)\n",
    "    print(f\"Adjustment model output: {test_pred[0].cpu().numpy()}\")\n",
    "    \n",
    "    absolute_pred = apply_adjustments(test_pred, test_metadata)\n",
    "    print(f\"Converted to absolute: Temp {absolute_pred[0,0].item():.0f}K, Tint {absolute_pred[0,1].item():.0f}\")\n",
    "    print(f\"Actual target: Temp {test_target[0].item():.0f}K, Tint {test_target[1].item():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db97cb1-ac24-4c6c-997c-8687fe77140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustment_loss(predictions, targets, metadata):\n",
    "    target_adjustments = calculate_adjustments(targets, metadata)\n",
    "    \n",
    "    loss = F.mse_loss(predictions, target_adjustments)\n",
    "    return loss\n",
    "\n",
    "adjustment_model.train()\n",
    "batch_images, batch_metadata, batch_targets = next(iter(train_loader))\n",
    "\n",
    "batch_images = batch_images.to(device)\n",
    "batch_metadata = batch_metadata.to(device)\n",
    "batch_targets = batch_targets.to(device)\n",
    "\n",
    "adjustment_optimizer.zero_grad()\n",
    "predicted_adjustments = adjustment_model(batch_images, batch_metadata)\n",
    "loss = adjustment_loss(predicted_adjustments, batch_targets, batch_metadata)\n",
    "\n",
    "print(f\"Initial batch loss: {loss.item():.4f}\")\n",
    "\n",
    "loss.backward()\n",
    "adjustment_optimizer.step()\n",
    "\n",
    "print(\"Model can train successfully!\")\n",
    "print(f\"Gradient flow: {any(p.grad is not None and p.grad.abs().sum() > 0 for p in adjustment_model.parameters())}\")\n",
    "\n",
    "adjustment_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = adjustment_model(test_image, test_metadata)\n",
    "    absolute_pred = apply_adjustments(test_pred, test_metadata)\n",
    "    print(f\"After one step - Predicted: Temp {absolute_pred[0,0].item():.0f}K, Tint {absolute_pred[0,1].item():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9041f0-79af-4b07-884f-211d207b4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "adjustment_model.train()\n",
    "train_losses = []\n",
    "num_epochs = 5  # Let's do 5 epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images, metadata, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        images = images.to(device)\n",
    "        metadata = metadata.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        adjustment_optimizer.zero_grad()\n",
    "        predicted_adjustments = adjustment_model(images, metadata)\n",
    "        loss = adjustment_loss(predicted_adjustments, targets, metadata)\n",
    "        \n",
    "        loss.backward()\n",
    "        adjustment_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete:\")\n",
    "    print(f\"- Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"- Time: {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    adjustment_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_image, test_metadata, test_target = train_dataset[0]\n",
    "        test_image = test_image.unsqueeze(0).to(device)\n",
    "        test_metadata = test_metadata.unsqueeze(0).to(device)\n",
    "        \n",
    "        prediction = adjustment_model(test_image, test_metadata)\n",
    "        absolute_pred = apply_adjustments(prediction, test_metadata)\n",
    "        \n",
    "        print(f\"Sample Prediction:\")\n",
    "        print(f\"As Shot:   Temp {test_metadata[0,0].item():.0f}K, Tint {test_metadata[0,1].item():.0f}\")\n",
    "        print(f\"Predicted: Temp {absolute_pred[0,0].item():.0f}K, Tint {absolute_pred[0,1].item():.0f}\")\n",
    "        print(f\"Actual:    Temp {test_target[0].item():.0f}K, Tint {test_target[1].item():.0f}\")\n",
    "        print(f\"Temp Error: {abs(absolute_pred[0,0].item() - test_target[0].item()):.0f}K\")\n",
    "\n",
    "print(f\"\\nTRAINING COMPLETE!\")\n",
    "print(f\"Final losses: {[f'{loss:.4f}' for loss in train_losses]}\")\n",
    "\n",
    "if train_losses[-1] < train_losses[0] * 0.8:  \n",
    "    print(\"EXCELLENT: Significant learning occurred!\")\n",
    "elif train_losses[-1] < train_losses[0]:\n",
    "    print(\"GOOD: Model is learning!\")\n",
    "else:\n",
    "    print(\"Model needs more tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348a16c-238f-4c55-b801-bc6ef2b8fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_adjustments = []\n",
    "tint_adjustments = []\n",
    "\n",
    "for i in range(min(100, len(train_df))):  \n",
    "    temp_adj = (train_df.iloc[i]['Temperature'] - train_df.iloc[i]['currTemp']) / 1000.0\n",
    "    tint_adj = (train_df.iloc[i]['Tint'] - train_df.iloc[i]['currTint']) / 10.0\n",
    "    temp_adjustments.append(temp_adj)\n",
    "    tint_adjustments.append(tint_adj)\n",
    "\n",
    "print(f\"Temperature adjustments: Min {min(temp_adjustments):.2f}, Max {max(temp_adjustments):.2f}, Mean {np.mean(temp_adjustments):.2f}\")\n",
    "print(f\"Tint adjustments: Min {min(tint_adjustments):.2f}, Max {max(tint_adjustments):.2f}, Mean {np.mean(tint_adjustments):.2f}\")\n",
    "\n",
    "class BetterAdjustmentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BetterAdjustmentModel, self).__init__()\n",
    "        \n",
    "        self.backbone = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        for param in list(self.backbone.parameters())[:-50]: \n",
    "            param.requires_grad = False\n",
    "            \n",
    "        image_feature_dim = 1280\n",
    "        \n",
    "        self.metadata_fc = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim + 128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.combined_fc[-1].weight.data.normal_(0, 0.01)\n",
    "        self.combined_fc[-1].bias.data.uniform_(-0.1, 0.1)  \n",
    "        \n",
    "    def forward(self, image, metadata):\n",
    "        image_features = self.backbone(image)\n",
    "        metadata_features = self.metadata_fc(metadata)\n",
    "        combined = torch.cat([image_features, metadata_features], dim=1)\n",
    "        adjustments = self.combined_fc(combined)\n",
    "        adjustments = self.tanh(adjustments)\n",
    "        return adjustments\n",
    "\n",
    "print(\"\\nCreating better model with batch normalization and careful initialization...\")\n",
    "better_model = BetterAdjustmentModel().to(device)\n",
    "better_optimizer = torch.optim.AdamW(better_model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "print(f\"Better model created with {sum(p.numel() for p in better_model.parameters()):,} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in better_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "better_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = better_model(test_image, test_metadata)\n",
    "    absolute_pred = apply_adjustments(test_pred, test_metadata)\n",
    "    print(f\"Better model initial prediction: Temp {absolute_pred[0,0].item():.0f}K, Tint {absolute_pred[0,1].item():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0607af9-740b-4706-92c2-8f95075a984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuick training test with better model...\")\n",
    "\n",
    "better_model.train()\n",
    "for i in range(3):  # Just 3 batches to see if it learns\n",
    "    batch_images, batch_metadata, batch_targets = next(iter(train_loader))\n",
    "    batch_images = batch_images.to(device)\n",
    "    batch_metadata = batch_metadata.to(device)\n",
    "    batch_targets = batch_targets.to(device)\n",
    "    \n",
    "    better_optimizer.zero_grad()\n",
    "    predicted_adjustments = better_model(batch_images, batch_metadata)\n",
    "    loss = adjustment_loss(predicted_adjustments, batch_targets, batch_metadata)\n",
    "    loss.backward()\n",
    "    better_optimizer.step()\n",
    "    \n",
    "    if i == 0:\n",
    "        print(f\"First batch loss: {loss.item():.4f}\")\n",
    "\n",
    "better_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = better_model(test_image, test_metadata)\n",
    "    absolute_pred = apply_adjustments(test_pred, test_metadata)\n",
    "    print(f\"After quick training: Temp {absolute_pred[0,0].item():.0f}K, Tint {absolute_pred[0,1].item():.0f}\")\n",
    "    print(f\"Actual target: Temp {test_target[0].item():.0f}K, Tint {test_target[1].item():.0f}\")\n",
    "    print(f\"Temp Error: {abs(absolute_pred[0,0].item() - test_target[0].item()):.0f}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e2d54-64f8-4e38-a641-0b1f17631c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRYING SIMPLER APPROACH: Basic CNN + Metadata\")\n",
    "\n",
    "class SimpleWhiteBalanceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleWhiteBalanceModel, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, image, metadata):\n",
    "        image_features = self.conv_layers(image)\n",
    "        image_features = image_features.view(image_features.size(0), -1)\n",
    "        \n",
    "        combined = torch.cat([image_features, metadata], dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output\n",
    "\n",
    "print(\"Creating simple model...\")\n",
    "simple_model = SimpleWhiteBalanceModel().to(device)\n",
    "simple_optimizer = torch.optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Simple model created with {sum(p.numel() for p in simple_model.parameters()):,} parameters\")\n",
    "\n",
    "simple_model.eval()\n",
    "with torch.no_grad():\n",
    "    simple_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    simple_dataset = WhiteBalanceDataset(\n",
    "        csv_path=os.path.join(dataset_path, 'Train', 'sliders.csv'),\n",
    "        images_dir=os.path.join(dataset_path, 'Train', 'images'),\n",
    "        transform=simple_transform,\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    simple_image, simple_metadata, simple_target = simple_dataset[0]\n",
    "    simple_image = simple_image.unsqueeze(0).to(device)\n",
    "    simple_metadata = simple_metadata.unsqueeze(0).to(device)\n",
    "    \n",
    "    simple_pred = simple_model(simple_image, simple_metadata)\n",
    "    print(f\"Simple model prediction: Temp {simple_pred[0,0].item():.0f}K, Tint {simple_pred[0,1].item():.0f}\")\n",
    "    print(f\"Actual target: Temp {simple_target[0].item():.0f}K, Tint {simple_target[1].item():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565066a9-8450-44e1-8b8c-9939ddae8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTRYING CLASSIFICATION APPROACH\")\n",
    "\n",
    "temp_bins = [2000, 3000, 4000, 5000, 6000, 7000, 8000, 10000, 15000, 50000]\n",
    "tint_bins = [-150, -100, -50, -20, 0, 20, 50, 100, 150]\n",
    "\n",
    "def values_to_classes(temp_values, tint_values):\n",
    "    \"\"\"Convert continuous values to class labels\"\"\"\n",
    "    temp_classes = torch.bucketize(temp_values, torch.tensor(temp_bins))\n",
    "    tint_classes = torch.bucketize(tint_values, torch.tensor(tint_bins))\n",
    "    return temp_classes, tint_classes\n",
    "\n",
    "def classes_to_values(temp_classes, tint_classes):\n",
    "    \"\"\"Convert class labels back to approximate values\"\"\"\n",
    "    temp_values = torch.tensor([temp_bins[min(c, len(temp_bins)-1)] for c in temp_classes])\n",
    "    tint_values = torch.tensor([tint_bins[min(c, len(tint_bins)-1)] for c in tint_classes])\n",
    "    return temp_values, tint_values\n",
    "\n",
    "test_temp = torch.tensor([4150.0, 2500.0, 12000.0])\n",
    "test_tint = torch.tensor([2.0, -30.0, 80.0])\n",
    "\n",
    "temp_classes, tint_classes = values_to_classes(test_temp, test_tint)\n",
    "temp_recon, tint_recon = classes_to_values(temp_classes, tint_classes)\n",
    "\n",
    "print(\"Classification approach test:\")\n",
    "print(f\"   Original: {test_temp.numpy()}, {test_tint.numpy()}\")\n",
    "print(f\"   Classes: {temp_classes.numpy()}, {tint_classes.numpy()}\")\n",
    "print(f\"   Reconstructed: {temp_recon.numpy()}, {tint_recon.numpy()}\")\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_temp_classes=len(temp_bins)+1, num_tint_classes=len(tint_bins)+1):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        \n",
    "        self.backbone = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280 + 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_temp_classes + num_tint_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_temp_classes = num_temp_classes\n",
    "        self.num_tint_classes = num_tint_classes\n",
    "        \n",
    "    def forward(self, image, metadata):\n",
    "        image_features = self.backbone(image)\n",
    "        combined = torch.cat([image_features, metadata], dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        \n",
    "        temp_logits = logits[:, :self.num_temp_classes]\n",
    "        tint_logits = logits[:, self.num_temp_classes:]\n",
    "        \n",
    "        return temp_logits, tint_logits\n",
    "\n",
    "print(\"\\nCreating classification model...\")\n",
    "class_model = ClassificationModel().to(device)\n",
    "class_optimizer = torch.optim.Adam(class_model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Classification model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68f6fa-b346-4033-8867-71f9132b856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING CLASSIFICATION MODEL\")\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, csv_path, images_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.temp_classes, self.tint_classes = values_to_classes(\n",
    "            torch.tensor(self.data['Temperature'].values),\n",
    "            torch.tensor(self.data['Tint'].values)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_id = row['id_global']\n",
    "        image_path = os.path.join(self.images_dir, f\"{image_id}.tif\")\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        metadata_features = [\n",
    "            row['currTemp'], row['currTint'], row['aperture'],\n",
    "            row['focalLength'], row['isoSpeedRating'], row['shutterSpeed'],\n",
    "            row['flashFired'], row['grayscale']\n",
    "        ]\n",
    "        metadata = torch.tensor(metadata_features, dtype=torch.float32)\n",
    "        \n",
    "        return image, metadata, self.temp_classes[idx], self.tint_classes[idx]\n",
    "\n",
    "class_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class_train_dataset = ClassificationDataset(\n",
    "    csv_path=os.path.join(dataset_path, 'Train', 'sliders.csv'),\n",
    "    images_dir=os.path.join(dataset_path, 'Train', 'images'),\n",
    "    transform=class_transform\n",
    ")\n",
    "\n",
    "class_train_loader = DataLoader(class_train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Classification dataset created: {len(class_train_dataset)} samples\")\n",
    "print(f\"Temperature classes: {len(temp_bins)+1} classes\")\n",
    "print(f\"Tint classes: {len(tint_bins)+1} classes\")\n",
    "\n",
    "for images, metadata, temp_classes, tint_classes in class_train_loader:\n",
    "    print(f\"Batch test:\")\n",
    "    print(f\"- Images: {images.shape}\")\n",
    "    print(f\"- Temp classes: {temp_classes[:5].numpy()}\")  # Show first 5\n",
    "    print(f\"- Tint classes: {tint_classes[:5].numpy()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe2745-d6b9-4693-810c-4c9fd07fec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class_model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 3\n",
    "\n",
    "print(f\"\\nSTARTING CLASSIFICATION TRAINING\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    temp_correct = 0\n",
    "    tint_correct = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images, metadata, temp_targets, tint_targets) in enumerate(tqdm(class_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        images = images.to(device)\n",
    "        metadata = metadata.to(device)\n",
    "        temp_targets = temp_targets.to(device)\n",
    "        tint_targets = tint_targets.to(device)\n",
    "        \n",
    "        class_optimizer.zero_grad()\n",
    "        temp_logits, tint_logits = class_model(images, metadata)\n",
    "        \n",
    "        temp_loss = criterion(temp_logits, temp_targets)\n",
    "        tint_loss = criterion(tint_logits, tint_targets)\n",
    "        total_loss = temp_loss + tint_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        class_optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        _, temp_preds = torch.max(temp_logits, 1)\n",
    "        _, tint_preds = torch.max(tint_logits, 1)\n",
    "        temp_correct += (temp_preds == temp_targets).sum().item()\n",
    "        tint_correct += (tint_preds == tint_targets).sum().item()\n",
    "        total_samples += temp_targets.size(0)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(class_train_loader)\n",
    "    temp_accuracy = 100.0 * temp_correct / total_samples\n",
    "    tint_accuracy = 100.0 * tint_correct / total_samples\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete:\")\n",
    "    print(f\"   - Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"   - Temp Accuracy: {temp_accuracy:.1f}%\")\n",
    "    print(f\"   - Tint Accuracy: {tint_accuracy:.1f}%\")\n",
    "    print(f\"   - Time: {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    class_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_image, test_metadata, test_temp_class, test_tint_class = class_train_dataset[0]\n",
    "        test_image = test_image.unsqueeze(0).to(device)\n",
    "        test_metadata = test_metadata.unsqueeze(0).to(device)\n",
    "        \n",
    "        temp_logits, tint_logits = class_model(test_image, test_metadata)\n",
    "        temp_pred_class = torch.argmax(temp_logits, 1)\n",
    "        tint_pred_class = torch.argmax(tint_logits, 1)\n",
    "        \n",
    "        temp_pred_value, tint_pred_value = classes_to_values(temp_pred_class.cpu(), tint_pred_class.cpu())\n",
    "        temp_actual_value, tint_actual_value = classes_to_values(\n",
    "            torch.tensor([test_temp_class]), torch.tensor([test_tint_class])\n",
    "        )\n",
    "        \n",
    "        print(f\"Sample Prediction:\")\n",
    "        print(f\"Predicted: Temp {temp_pred_value[0].item():.0f}K, Tint {tint_pred_value[0].item():.0f}\")\n",
    "        print(f\"Actual:    Temp {temp_actual_value[0].item():.0f}K, Tint {tint_actual_value[0].item():.0f}\")\n",
    "\n",
    "print(f\"\\nCLASSIFICATION TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8b8be-2906-40d9-8243-522df33336ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_model.eval()\n",
    "all_temp_preds = []\n",
    "all_temp_actual = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(min(50, len(class_train_dataset))):  # Sample 50\n",
    "        image, metadata, temp_class, tint_class = class_train_dataset[i]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        metadata = metadata.unsqueeze(0).to(device)\n",
    "        \n",
    "        temp_logits, tint_logits = class_model(image, metadata)\n",
    "        temp_pred_class = torch.argmax(temp_logits, 1)\n",
    "        \n",
    "        temp_pred, _ = classes_to_values(temp_pred_class.cpu(), torch.tensor([0]))\n",
    "        temp_actual, _ = classes_to_values(torch.tensor([temp_class]), torch.tensor([0]))\n",
    "        \n",
    "        all_temp_preds.append(temp_pred[0].item())\n",
    "        all_temp_actual.append(temp_actual[0].item())\n",
    "\n",
    "print(f\"Sample predictions vs actual:\")\n",
    "for i in range(10):\n",
    "    print(f\"  {all_temp_preds[i]:5.0f}K vs {all_temp_actual[i]:5.0f}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b8212-d514-43c7-a68b-87c4bbba0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CREATING SUBMISSION FILE\")\n",
    "\n",
    "validation_dataset = WhiteBalanceDataset(\n",
    "    csv_path=os.path.join(dataset_path, 'Validation', 'sliders_input.csv'),\n",
    "    images_dir=os.path.join(dataset_path, 'Validation', 'images'),\n",
    "    transform=class_transform,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "class_model.eval()\n",
    "predictions = []\n",
    "image_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, metadata, ids in tqdm(validation_loader, desc=\"Predicting\"):\n",
    "        images = images.to(device)\n",
    "        metadata = metadata.to(device)\n",
    "        \n",
    "        temp_logits, tint_logits = class_model(images, metadata)\n",
    "        temp_pred_class = torch.argmax(temp_logits, 1)\n",
    "        tint_pred_class = torch.argmax(tint_logits, 1)\n",
    "        \n",
    "        temp_pred_values, tint_pred_values = classes_to_values(\n",
    "            temp_pred_class.cpu(), tint_pred_class.cpu()\n",
    "        )\n",
    "        \n",
    "        predictions.extend(zip(temp_pred_values.numpy(), tint_pred_values.numpy()))\n",
    "        image_ids.extend(ids)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id_global': image_ids,\n",
    "    'Temperature': [int(round(pred[0])) for pred in predictions],\n",
    "    'Tint': [int(round(pred[1])) for pred in predictions]\n",
    "})\n",
    "\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"Temperature range: {submission_df['Temperature'].min()} to {submission_df['Temperature'].max()}\")\n",
    "print(f\"Tint range: {submission_df['Tint'].min()} to {submission_df['Tint'].max()}\")\n",
    "\n",
    "submission_file = 'white_balance_submission.csv'\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"Submission saved as: {submission_file}\")\n",
    "\n",
    "print(\"\\nSUBMISSION PREVIEW:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813dcaa-63f8-40e3-9f39-28426f33a1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
